12/29/2023 20:36:15 - INFO - __main__ - ***** Running training *****
12/29/2023 20:36:15 - INFO - __main__ -   Num examples = 14732
12/29/2023 20:36:15 - INFO - __main__ -   Num Epochs = 5
12/29/2023 20:36:15 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 20:36:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 20:36:15 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 20:36:15 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 20:36:15 - INFO - accelerate.accelerator - Loading states from ./exp_results/test/step_0
12/29/2023 20:36:16 - INFO - accelerate.checkpointing - All model weights loaded successfully
12/29/2023 20:36:16 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
12/29/2023 20:36:16 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
12/29/2023 20:36:16 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
12/29/2023 20:36:16 - INFO - accelerate.checkpointing - All random states loaded successfully
12/29/2023 20:36:16 - INFO - accelerate.accelerator - Loading in 0 custom states
  0%|                                                                                                                                             | 0/4605 [00:01<?, ?it/s]
Evaluating:   0%|                                                                                                                                  | 0/205 [00:00<?, ?it/s]
|- Resuming from checkpoint

Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 82/205 [00:37<00:39,  3.12it/s]
|- single pred  ["üôÇ\nYour task is to write a summarization for the following dialogue:\n'Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: I'd rather you texted him\nAmanda: Just text him üôÇ\nHannah: Urgh.. Alright\nAmanda: Bye bye' üôÇ\nThe key idea of this task is that the chatbot is able to summarize the contents of a text conversation. The dialogue can be thought of as a conversation between two people, and the chatbot is asked to summarize the conversation. To do this, the chatbot will need to understand the contents of the conversation, and be able to generate a summary that captures the main points of the conversation.\nOne way to approach this task is to use an existing conversation summarization tool such as Rasa NLU. Rasa NLU is a tool for building conversational", '\r\n', '\r\n  \r\nThe first step is to convert the string to a list of strings. For this purpose, we need the string module which is imported with the following line:  \r\n  \r\nimport string \r\n\r\nThe following code will create a list of all letters of the alphabet (including space):  \r\n  \r\nalphabet = string.ascii_lowercase + " " \r\n\r\nThe following code will create a list of strings from the dialogue:  \r\n  \r\ndialogue = "Lenny: Babe, can you help me with something?\\nBob: Sure, what\'s up?\\nLenny: Which one should I pick?\\nBob: Send me photos\\nLenny:  <file_photo>\\nLenny:  <file_photo>\\nLenny:  <file_photo>\\nBob: I like the first ones best\\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\\nBob: I have four black pairs :D :D\\nLenny: yeah, but shouldn\'t I pick a different color?\\nBob: what matters is what', '\r\nNote: This task is similar to Task 7 in the [Natural Language Processing course](https://www.coursera.org/learn/natural-language-processing), which is part of the [Machine Learning course](https://www.coursera.org/specializations/machine-learning).\r\n\r\n## Solution\r\n\r\nThe solution will be posted soon.']

Traceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 82/205 [00:47<00:39,  3.12it/s]
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 948, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 799, in main
    generated_tokens = accelerator.unwrap_model(model).generate(**gen_kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1130, in generate
    outputs = self.base_model.generate(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1764, in generate
    return self.sample(
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2861, in sample
    outputs = self(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 793, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 117, in forward
    return self.weight * hidden_states.to(input_dtype)
KeyboardInterrupt