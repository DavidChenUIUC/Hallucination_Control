12/29/2023 16:55:37 - INFO - __main__ - ***** Running training *****
12/29/2023 16:55:37 - INFO - __main__ -   Num examples = 14732
12/29/2023 16:55:37 - INFO - __main__ -   Num Epochs = 5
12/29/2023 16:55:37 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 16:55:37 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 16:55:37 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 16:55:37 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 16:55:38 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_high_lr_linear_decay/step_0
12/29/2023 16:55:45 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_high_lr_linear_decay/step_0/model.safetensors
12/29/2023 16:55:45 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 16:55:45 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 16:55:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler.bin
12/29/2023 16:55:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 16:55:45 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 16:55:46 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_high_lr_linear_decay/step_0
12/29/2023 16:56:02 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_high_lr_linear_decay/step_0/model.safetensors
12/29/2023 16:56:02 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 16:56:02 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 16:56:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler.bin
12/29/2023 16:56:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 16:56:02 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 16:56:03 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_high_lr_linear_decay/step_0
12/29/2023 16:56:10 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_high_lr_linear_decay/step_0/model.safetensors
12/29/2023 16:56:10 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 16:56:10 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 16:56:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler.bin
12/29/2023 16:56:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 16:56:10 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_high_lr_linear_decay/step_0/random_states_0.pkl
  0%|                                                                                                                                  | 1/4605 [00:34<43:57:03, 34.37s/it]
Epoch: 0 | Step: 1 | Loss: 11.468478202819824
Epoch: 0 | Step: 1 | Loss: 11.471742630004883

  0%|                                                                                                                                  | 2/4605 [00:39<21:45:12, 17.01s/it]
Epoch: 0 | Step: 1 | Loss: 11.470683097839355
Epoch: 0 | Step: 2 | Loss: 11.452973365783691
Epoch: 0 | Step: 2 | Loss: 11.452966690063477
Epoch: 0 | Step: 2 | Loss: 11.457958221435547

  0%|                                                                                                                                  | 3/4605 [00:44<14:39:52, 11.47s/it]
Epoch: 0 | Step: 3 | Loss: 11.433927536010742
Epoch: 0 | Step: 3 | Loss: 11.437705039978027
Epoch: 0 | Step: 3 | Loss: 11.432554244995117

  0%|                                                                                                                                  | 4/4605 [00:48<11:20:23,  8.87s/it]
Epoch: 0 | Step: 4 | Loss: 11.397432327270508
Epoch: 0 | Step: 4 | Loss: 11.396767616271973
Epoch: 0 | Step: 4 | Loss: 11.400214195251465
Epoch: 0 | Step: 4 | Loss: 11.398604393005371

  0%|▏                                                                                                                                  | 5/4605 [00:53<9:30:19,  7.44s/it]
Epoch: 0 | Step: 5 | Loss: 11.355822563171387
Epoch: 0 | Step: 5 | Loss: 11.358586311340332

  0%|▏                                                                                                                                  | 6/4605 [00:58<8:24:11,  6.58s/it]
Epoch: 0 | Step: 6 | Loss: 11.303502082824707
Epoch: 0 | Step: 6 | Loss: 11.30607795715332
Epoch: 0 | Step: 6 | Loss: 11.301606178283691
Epoch: 0 | Step: 6 | Loss: 11.302026748657227

  0%|▏                                                                                                                                  | 7/4605 [01:03<7:42:20,  6.03s/it]
Epoch: 0 | Step: 7 | Loss: 11.24489688873291
Epoch: 0 | Step: 7 | Loss: 11.24588680267334

  0%|▏                                                                                                                                  | 8/4605 [01:08<7:14:53,  5.68s/it]
Epoch: 0 | Step: 8 | Loss: 11.182737350463867
Epoch: 0 | Step: 8 | Loss: 11.179914474487305
Epoch: 0 | Step: 8 | Loss: 11.182783126831055

  0%|▎                                                                                                                                  | 9/4605 [01:13<6:56:43,  5.44s/it]
Epoch: 0 | Step: 9 | Loss: 11.113231658935547
Epoch: 0 | Step: 9 | Loss: 11.112460136413574
Epoch: 0 | Step: 9 | Loss: 11.11115837097168

  0%|▎                                                                                                                                 | 10/4605 [01:18<6:44:35,  5.28s/it]
Epoch: 0 | Step: 10 | Loss: 11.039926528930664
Epoch: 0 | Step: 10 | Loss: 11.040337562561035
Epoch: 0 | Step: 10 | Loss: 11.041465759277344

  0%|▎                                                                                                                                 | 11/4605 [01:23<6:36:08,  5.17s/it]
Epoch: 0 | Step: 11 | Loss: 10.967902183532715
Epoch: 0 | Step: 11 | Loss: 10.966239929199219
Epoch: 0 | Step: 11 | Loss: 10.968961715698242
Epoch: 0 | Step: 11 | Loss: 10.967853546142578

  0%|▎                                                                                                                                 | 12/4605 [01:28<6:30:21,  5.10s/it]
Epoch: 0 | Step: 12 | Loss: 10.889266014099121
Epoch: 0 | Step: 12 | Loss: 10.887784957885742

  0%|▎                                                                                                                                 | 13/4605 [01:33<6:26:19,  5.05s/it]
Epoch: 0 | Step: 13 | Loss: 10.811018943786621
Epoch: 0 | Step: 13 | Loss: 10.815240859985352
Epoch: 0 | Step: 13 | Loss: 10.812660217285156
Epoch: 0 | Step: 13 | Loss: 10.813811302185059

  0%|▍                                                                                                                                 | 14/4605 [01:38<6:23:59,  5.02s/it]
Epoch: 0 | Step: 14 | Loss: 10.729812622070312
Epoch: 0 | Step: 14 | Loss: 10.72878646850586

  0%|▍                                                                                                                                 | 15/4605 [01:43<6:21:55,  4.99s/it]
Epoch: 0 | Step: 15 | Loss: 10.649334907531738
Epoch: 0 | Step: 15 | Loss: 10.649341583251953
Epoch: 0 | Step: 15 | Loss: 10.648808479309082
Epoch: 0 | Step: 15 | Loss: 10.64599895477295

  0%|▍                                                                                                                                 | 16/4605 [01:48<6:20:45,  4.98s/it]
Epoch: 0 | Step: 16 | Loss: 10.563041687011719
Epoch: 0 | Step: 16 | Loss: 10.564903259277344

  0%|▍                                                                                                                                 | 17/4605 [01:53<6:19:30,  4.96s/it]
Epoch: 0 | Step: 17 | Loss: 10.4760103225708
Epoch: 0 | Step: 17 | Loss: 10.475677490234375
Epoch: 0 | Step: 17 | Loss: 10.475584983825684
Epoch: 0 | Step: 17 | Loss: 10.47655963897705

  0%|▌                                                                                                                                 | 18/4605 [01:57<6:18:50,  4.96s/it]
Epoch: 0 | Step: 18 | Loss: 10.384183883666992
Epoch: 0 | Step: 18 | Loss: 10.386124610900879

  0%|▌                                                                                                                                 | 19/4605 [02:02<6:18:21,  4.95s/it]
Epoch: 0 | Step: 19 | Loss: 10.290212631225586
Epoch: 0 | Step: 19 | Loss: 10.292373657226562
Epoch: 0 | Step: 19 | Loss: 10.290711402893066
Epoch: 0 | Step: 19 | Loss: 10.293355941772461

  0%|▌                                                                                                                                 | 20/4605 [02:07<6:17:57,  4.95s/it]
Epoch: 0 | Step: 20 | Loss: 10.195975303649902
Epoch: 0 | Step: 20 | Loss: 10.193511009216309

  0%|▌                                                                                                                                 | 21/4605 [02:12<6:17:37,  4.94s/it]
Epoch: 0 | Step: 21 | Loss: 10.100255012512207
Epoch: 0 | Step: 21 | Loss: 10.096134185791016
Epoch: 0 | Step: 21 | Loss: 10.097554206848145

  0%|▌                                                                                                                                 | 22/4605 [02:17<6:17:28,  4.94s/it]
Epoch: 0 | Step: 22 | Loss: 9.994413375854492
Epoch: 0 | Step: 22 | Loss: 9.992048263549805
Epoch: 0 | Step: 22 | Loss: 9.992325782775879

  0%|▋                                                                                                                                 | 23/4605 [02:22<6:17:16,  4.94s/it]
Epoch: 0 | Step: 23 | Loss: 9.884050369262695
Epoch: 0 | Step: 23 | Loss: 9.881508827209473
Epoch: 0 | Step: 23 | Loss: 9.880615234375

  1%|▋                                                                                                                                 | 24/4605 [02:27<6:17:06,  4.94s/it]
Epoch: 0 | Step: 24 | Loss: 9.76953125
Epoch: 0 | Step: 24 | Loss: 9.766143798828125
Epoch: 0 | Step: 24 | Loss: 9.766651153564453

  1%|▋                                                                                                                                 | 25/4605 [02:32<6:17:00,  4.94s/it]
Epoch: 0 | Step: 25 | Loss: 9.639512062072754
Epoch: 0 | Step: 25 | Loss: 9.640026092529297
Epoch: 0 | Step: 25 | Loss: 9.639988899230957

  1%|▋                                                                                                                                 | 26/4605 [02:37<6:16:45,  4.94s/it]
Epoch: 0 | Step: 26 | Loss: 9.501051902770996
Epoch: 0 | Step: 26 | Loss: 9.505280494689941
Epoch: 0 | Step: 26 | Loss: 9.501622200012207
Epoch: 0 | Step: 26 | Loss: 9.501567840576172

  1%|▊                                                                                                                                 | 27/4605 [02:42<6:16:49,  4.94s/it]
Epoch: 0 | Step: 27 | Loss: 9.351733207702637
Epoch: 0 | Step: 27 | Loss: 9.35518741607666

  1%|▊                                                                                                                                 | 28/4605 [02:47<6:16:50,  4.94s/it]
Epoch: 0 | Step: 28 | Loss: 9.172255516052246
Epoch: 0 | Step: 28 | Loss: 9.17628002166748

  1%|▊                                                                                                                                 | 29/4605 [02:52<6:16:47,  4.94s/it]
Epoch: 0 | Step: 28 | Loss: 9.176462173461914
Epoch: 0 | Step: 29 | Loss: 8.978630065917969
Epoch: 0 | Step: 29 | Loss: 8.970451354980469
Epoch: 0 | Step: 29 | Loss: 8.978918075561523

  1%|▊                                                                                                                                 | 30/4605 [02:57<6:16:57,  4.94s/it]
Epoch: 0 | Step: 30 | Loss: 8.763538360595703
Epoch: 0 | Step: 30 | Loss: 8.76308822631836

  1%|▉                                                                                                                                 | 31/4605 [03:02<6:17:00,  4.95s/it]
Epoch: 0 | Step: 30 | Loss: 8.758537292480469
Epoch: 0 | Step: 31 | Loss: 8.638007164001465
Epoch: 0 | Step: 31 | Loss: 8.644362449645996
Epoch: 0 | Step: 31 | Loss: 8.637940406799316

  1%|▉                                                                                                                                 | 32/4605 [03:07<6:16:55,  4.95s/it]
Epoch: 0 | Step: 32 | Loss: 8.959117889404297
Epoch: 0 | Step: 32 | Loss: 8.975059509277344

  1%|▉                                                                                                                                 | 33/4605 [03:12<6:16:52,  4.95s/it]
Epoch: 0 | Step: 32 | Loss: 8.977129936218262
Epoch: 0 | Step: 33 | Loss: 9.493696212768555
Epoch: 0 | Step: 33 | Loss: 9.56194019317627
Epoch: 0 | Step: 33 | Loss: 9.517542839050293

  1%|▉                                                                                                                                 | 34/4605 [03:17<6:16:53,  4.95s/it]
Epoch: 0 | Step: 34 | Loss: 8.8685884475708
Epoch: 0 | Step: 34 | Loss: 8.868372917175293

  1%|▉                                                                                                                                 | 35/4605 [03:21<6:16:50,  4.95s/it]
Epoch: 0 | Step: 34 | Loss: 8.891707420349121
Epoch: 0 | Step: 35 | Loss: 8.410083770751953
Epoch: 0 | Step: 35 | Loss: 8.410205841064453
Epoch: 0 | Step: 35 | Loss: 8.396081924438477

  1%|█                                                                                                                                 | 36/4605 [03:26<6:16:36,  4.95s/it]
Epoch: 0 | Step: 36 | Loss: 8.21848201751709
Epoch: 0 | Step: 36 | Loss: 8.227110862731934

  1%|█                                                                                                                                 | 37/4605 [03:31<6:16:26,  4.94s/it]
Epoch: 0 | Step: 36 | Loss: 8.227178573608398
Epoch: 0 | Step: 37 | Loss: 8.080405235290527
Epoch: 0 | Step: 37 | Loss: 8.091774940490723
Epoch: 0 | Step: 37 | Loss: 8.086186408996582

  1%|█                                                                                                                                 | 38/4605 [03:36<6:16:22,  4.94s/it]
Epoch: 0 | Step: 38 | Loss: 7.94013786315918
Epoch: 0 | Step: 38 | Loss: 7.947098255157471
Epoch: 0 | Step: 38 | Loss: 7.951590538024902

  1%|█                                                                                                                                 | 39/4605 [03:41<6:16:19,  4.95s/it]
Epoch: 0 | Step: 39 | Loss: 7.792019844055176
Epoch: 0 | Step: 39 | Loss: 7.77250337600708
Epoch: 0 | Step: 39 | Loss: 7.7905802726745605
  1%|█▏                                                                                                                                | 40/4605 [03:46<6:16:23,  4.95s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 916, in <module>
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 714, in main
    for step, batch in enumerate(active_dataloader):
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1073, in forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 100, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 796, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 388, in forward
    value_states = self.v_proj(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 311, in forward
    output = lora_B(lora_A(dropout(x)))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Epoch: 0 | Step: 40 | Loss: 7.6338982582092285