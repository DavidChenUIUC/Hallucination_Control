Train dataset size: 14732
Test dataset size: 819
Map: 100%|██████████████████████████████████████████████████████████████████████████| 15551/15551 [00:01<00:00, 15112.60 examples/s]
Max source length: 255
Max target length: 50
Map: 100%|██████████████████████████████████████████████████████████████████████████| 15551/15551 [00:00<00:00, 61412.69 examples/s]
Map:  68%|██████████████████████████████████████████████████▉                        | 10000/14732 [00:01<00:00, 7935.49 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████| 14732/14732 [00:01<00:00, 7544.09 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 3637.30 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 818/818 [00:00<00:00, 7896.71 examples/s]
Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████| 14732/14732 [00:00<00:00, 591064.62 examples/s]
Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████| 819/819 [00:00<00:00, 235363.82 examples/s]




Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████| 12/12 [00:09<00:00,  1.32it/s]
trainable params: 18,874,368 || all params: 11,154,206,720 || trainable%: 0.16921300163961817
{'adafactor': False,
 'adam_beta1': 0.9,
 'adam_beta2': 0.999,
 'adam_epsilon': 1e-08,
 'auto_find_batch_size': True,
 'bf16': False,
 'bf16_full_eval': False,
 'data_seed': None,
 'dataloader_drop_last': False,
 'dataloader_num_workers': 0,
 'dataloader_persistent_workers': False,
 'dataloader_pin_memory': True,
 'ddp_backend': None,
 'ddp_broadcast_buffers': None,
 'ddp_bucket_cap_mb': None,
 'ddp_find_unused_parameters': None,
 'ddp_timeout': 1800,
 'debug': [],
 'deepspeed': None,
 'disable_tqdm': False,
 'dispatch_batches': None,
 'do_eval': False,
 'do_predict': False,
 'do_train': False,
 'eval_accumulation_steps': None,
 'eval_delay': 0,
 'eval_steps': None,
 'evaluation_strategy': 'no',
 'fp16': False,
 'fp16_backend': 'auto',
 'fp16_full_eval': False,
 'fp16_opt_level': 'O1',
 'fsdp': [],
 'fsdp_config': {'min_num_params': 0,
                 'xla': False,
                 'xla_fsdp_grad_ckpt': False},
 'fsdp_min_num_params': 0,
 'fsdp_transformer_layer_cls_to_wrap': None,
 'full_determinism': False,
 'generation_config': None,
 'generation_max_length': None,
 'generation_num_beams': None,
 'gradient_accumulation_steps': 1,
 'gradient_checkpointing': False,
 'gradient_checkpointing_kwargs': None,
 'greater_is_better': None,
 'group_by_length': False,
 'half_precision_backend': 'auto',
 'hub_always_push': False,
 'hub_model_id': None,
 'hub_private_repo': False,
 'hub_strategy': 'every_save',
 'hub_token': '<HUB_TOKEN>',
 'ignore_data_skip': False,
 'include_inputs_for_metrics': False,
 'include_num_input_tokens_seen': False,
 'include_tokens_per_second': False,
 'jit_mode_eval': False,
 'label_names': None,
 'label_smoothing_factor': 0.0,
 'learning_rate': 0.001,
 'length_column_name': 'length',
 'load_best_model_at_end': False,
 'local_rank': 0,
 'log_level': 'passive',
 'log_level_replica': 'warning',
 'log_on_each_node': True,
 'logging_dir': 'test_lora-flan-t5-xxl-1e-3_linear_warm0.1_seed1234/logs',
 'logging_first_step': False,
 'logging_nan_inf_filter': True,
 'logging_steps': 1,
 'logging_strategy': 'steps',
 'lr_scheduler_kwargs': {},
 'lr_scheduler_type': 'linear',
 'max_grad_norm': 1.0,
 'max_steps': -1,
 'metric_for_best_model': None,
 'mp_parameters': '',
 'neftune_noise_alpha': None,
 'no_cuda': False,
 'num_train_epochs': 5,
 'optim': 'adamw_torch',
 'optim_args': None,
 'output_dir': 'test_lora-flan-t5-xxl-1e-3_linear_warm0.1_seed1234',
 'overwrite_output_dir': False,
 'past_index': -1,
 'per_device_eval_batch_size': 32,
 'per_device_train_batch_size': 32,
 'per_gpu_eval_batch_size': None,
 'per_gpu_train_batch_size': None,
 'predict_with_generate': False,
 'prediction_loss_only': False,
 'push_to_hub': False,
 'push_to_hub_model_id': None,
 'push_to_hub_organization': None,
 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',
 'ray_scope': 'last',
 'remove_unused_columns': True,
 'report_to': ['wandb'],
 'resume_from_checkpoint': None,
 'run_name': 'test_lora-flan-t5-xxl-1e-3_linear_warm0.1_seed1234',
 'save_on_each_node': False,
 'save_only_model': False,
 'save_safetensors': True,
 'save_steps': 500,
 'save_strategy': 'steps',
 'save_total_limit': None,
 'seed': 42,
 'skip_memory_metrics': True,
 'sortish_sampler': False,
 'split_batches': False,
 'tf32': None,
 'torch_compile': False,
 'torch_compile_backend': None,
 'torch_compile_mode': None,
 'torchdynamo': None,
 'tpu_metrics_debug': False,
 'tpu_num_cores': None,
 'use_cpu': False,
 'use_ipex': False,
 'use_legacy_prediction_loop': False,
 'use_mps_device': False,
 'warmup_ratio': 0.1,
 'warmup_steps': 0,
 'weight_decay': 0.0}
|- *** Evaluate ***
/home/cwtang/.local/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")


