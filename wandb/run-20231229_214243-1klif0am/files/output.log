12/29/2023 21:42:43 - INFO - __main__ - ***** Running training *****
12/29/2023 21:42:43 - INFO - __main__ -   Num examples = 14732
12/29/2023 21:42:43 - INFO - __main__ -   Num Epochs = 5
12/29/2023 21:42:43 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 21:42:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 21:42:43 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 21:42:43 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 21:42:45 - INFO - accelerate.accelerator - Saving current state to exp_results/no_prompt_lr8e-5_decay0.1/step_0
12/29/2023 21:42:53 - INFO - accelerate.checkpointing - Model weights saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/model.safetensors
12/29/2023 21:42:53 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/optimizer.bin
12/29/2023 21:42:53 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/scheduler.bin
12/29/2023 21:42:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/sampler.bin
12/29/2023 21:42:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/sampler_1.bin
12/29/2023 21:42:53 - INFO - accelerate.checkpointing - Random states saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/random_states_0.pkl
tokenizer config file saved in exp_results/no_prompt_lr8e-5_decay0.1/tokenizer_config.json
Special tokens file saved in exp_results/no_prompt_lr8e-5_decay0.1/special_tokens_map.json
12/29/2023 21:42:55 - INFO - accelerate.accelerator - Saving current state to exp_results/no_prompt_lr8e-5_decay0.1/step_0
12/29/2023 21:43:10 - INFO - accelerate.checkpointing - Model weights saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/model.safetensors
12/29/2023 21:43:10 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/optimizer.bin
12/29/2023 21:43:10 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/scheduler.bin
12/29/2023 21:43:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/sampler.bin
12/29/2023 21:43:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/sampler_1.bin
12/29/2023 21:43:10 - INFO - accelerate.checkpointing - Random states saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/random_states_0.pkl
tokenizer config file saved in exp_results/no_prompt_lr8e-5_decay0.1/tokenizer_config.json
Special tokens file saved in exp_results/no_prompt_lr8e-5_decay0.1/special_tokens_map.json
12/29/2023 21:43:13 - INFO - accelerate.accelerator - Saving current state to exp_results/no_prompt_lr8e-5_decay0.1/step_0
12/29/2023 21:43:26 - INFO - accelerate.checkpointing - Model weights saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/model.safetensors
12/29/2023 21:43:26 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/optimizer.bin
12/29/2023 21:43:26 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/scheduler.bin
12/29/2023 21:43:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/sampler.bin
12/29/2023 21:43:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/sampler_1.bin
12/29/2023 21:43:26 - INFO - accelerate.checkpointing - Random states saved in exp_results/no_prompt_lr8e-5_decay0.1/step_0/random_states_0.pkl
tokenizer config file saved in exp_results/no_prompt_lr8e-5_decay0.1/tokenizer_config.json
Special tokens file saved in exp_results/no_prompt_lr8e-5_decay0.1/special_tokens_map.json
  0%|                                                                                                                                  | 1/4605 [00:45<58:21:16, 45.63s/it]
Epoch: 0 | Step: 1 | Loss: 19.721017837524414
Epoch: 0 | Step: 1 | Loss: 21.85786247253418

  0%|                                                                                                                                  | 2/4605 [00:50<27:41:44, 21.66s/it]
Epoch: 0 | Step: 1 | Loss: 18.058454513549805
Epoch: 0 | Step: 2 | Loss: 20.653146743774414
Epoch: 0 | Step: 2 | Loss: 17.31097412109375

  0%|                                                                                                                                  | 3/4605 [00:55<17:54:05, 14.00s/it]
Epoch: 0 | Step: 2 | Loss: 20.574342727661133
Epoch: 0 | Step: 3 | Loss: 22.778446197509766
Epoch: 0 | Step: 3 | Loss: 20.775985717773438
Epoch: 0 | Step: 3 | Loss: 18.37032699584961

  0%|                                                                                                                                  | 4/4605 [01:00<13:18:22, 10.41s/it]
Epoch: 0 | Step: 4 | Loss: 17.318601608276367
Epoch: 0 | Step: 4 | Loss: 20.776840209960938

  0%|▏                                                                                                                                 | 5/4605 [01:05<10:46:09,  8.43s/it]
Epoch: 0 | Step: 4 | Loss: 21.35529899597168
Epoch: 0 | Step: 5 | Loss: 17.196495056152344
Epoch: 0 | Step: 5 | Loss: 13.555601119995117
Epoch: 0 | Step: 5 | Loss: 21.921688079833984

  0%|▏                                                                                                                                  | 6/4605 [01:10<9:14:27,  7.23s/it]
Epoch: 0 | Step: 6 | Loss: 15.276969909667969
Epoch: 0 | Step: 6 | Loss: 18.40325164794922

  0%|▏                                                                                                                                  | 7/4605 [01:15<8:16:26,  6.48s/it]
Epoch: 0 | Step: 6 | Loss: 18.093334197998047
Epoch: 0 | Step: 7 | Loss: 17.556337356567383

  0%|▏                                                                                                                                  | 8/4605 [01:19<7:38:29,  5.98s/it]
Epoch: 0 | Step: 7 | Loss: 13.860784530639648
Epoch: 0 | Step: 7 | Loss: 18.217069625854492
Epoch: 0 | Step: 8 | Loss: 19.586332321166992
Epoch: 0 | Step: 8 | Loss: 19.243236541748047

  0%|▎                                                                                                                                  | 9/4605 [01:24<7:13:10,  5.66s/it]
Epoch: 0 | Step: 8 | Loss: 18.77366065979004
Epoch: 0 | Step: 9 | Loss: 18.94064712524414
Epoch: 0 | Step: 9 | Loss: 20.10642433166504
Epoch: 0 | Step: 9 | Loss: 20.672527313232422

  0%|▎                                                                                                                                 | 10/4605 [01:29<6:56:18,  5.44s/it]
Epoch: 0 | Step: 10 | Loss: 16.02310562133789
Epoch: 0 | Step: 10 | Loss: 22.675708770751953

  0%|▎                                                                                                                                 | 11/4605 [01:34<6:44:34,  5.28s/it]
Epoch: 0 | Step: 10 | Loss: 18.41510009765625
Epoch: 0 | Step: 11 | Loss: 16.928998947143555

  0%|▎                                                                                                                                 | 12/4605 [01:39<6:36:37,  5.18s/it]
Epoch: 0 | Step: 11 | Loss: 20.447040557861328
Epoch: 0 | Step: 11 | Loss: 21.875986099243164
Epoch: 0 | Step: 12 | Loss: 19.214847564697266
Epoch: 0 | Step: 12 | Loss: 17.741941452026367

  0%|▎                                                                                                                                 | 13/4605 [01:44<6:31:15,  5.11s/it]
Epoch: 0 | Step: 12 | Loss: 21.23813819885254
Epoch: 0 | Step: 13 | Loss: 20.281064987182617
Epoch: 0 | Step: 13 | Loss: 20.263708114624023

  0%|▍                                                                                                                                 | 14/4605 [01:49<6:27:24,  5.06s/it]
Epoch: 0 | Step: 13 | Loss: 19.98197364807129
Epoch: 0 | Step: 14 | Loss: 18.37919044494629
Epoch: 0 | Step: 14 | Loss: 21.023042678833008

  0%|▍                                                                                                                                 | 15/4605 [01:54<6:24:51,  5.03s/it]
Epoch: 0 | Step: 14 | Loss: 19.073230743408203
Epoch: 0 | Step: 15 | Loss: 17.993915557861328
Epoch: 0 | Step: 15 | Loss: 21.741352081298828

  0%|▍                                                                                                                                 | 16/4605 [01:59<6:22:59,  5.01s/it]
Epoch: 0 | Step: 15 | Loss: 19.537717819213867
Epoch: 0 | Step: 16 | Loss: 16.531435012817383
Epoch: 0 | Step: 16 | Loss: 20.2725830078125

  0%|▍                                                                                                                                 | 17/4605 [02:04<6:21:48,  4.99s/it]
Epoch: 0 | Step: 16 | Loss: 21.115375518798828
Epoch: 0 | Step: 17 | Loss: 17.47844696044922
Epoch: 0 | Step: 17 | Loss: 21.068384170532227

  0%|▌                                                                                                                                 | 18/4605 [02:09<6:21:00,  4.98s/it]
Epoch: 0 | Step: 17 | Loss: 19.956819534301758
Epoch: 0 | Step: 18 | Loss: 21.111961364746094
Epoch: 0 | Step: 18 | Loss: 19.789335250854492
Epoch: 0 | Step: 18 | Loss: 16.665264129638672

  0%|▌                                                                                                                                 | 19/4605 [02:14<6:20:28,  4.98s/it]
Epoch: 0 | Step: 19 | Loss: 19.141510009765625
Epoch: 0 | Step: 19 | Loss: 20.186189651489258

  0%|▌                                                                                                                                 | 20/4605 [02:19<6:20:10,  4.98s/it]
Epoch: 0 | Step: 19 | Loss: 20.4256649017334
Epoch: 0 | Step: 20 | Loss: 16.922929763793945
Epoch: 0 | Step: 20 | Loss: 18.542640686035156
Epoch: 0 | Step: 20 | Loss: 19.985231399536133

  0%|▌                                                                                                                                 | 21/4605 [02:24<6:19:56,  4.97s/it]
Epoch: 0 | Step: 21 | Loss: 15.569231986999512
Epoch: 0 | Step: 21 | Loss: 19.97559928894043

  0%|▌                                                                                                                                 | 22/4605 [02:29<6:19:50,  4.97s/it]
Epoch: 0 | Step: 21 | Loss: 21.768402099609375
Epoch: 0 | Step: 22 | Loss: 15.992792129516602
Epoch: 0 | Step: 22 | Loss: 14.879250526428223
Epoch: 0 | Step: 22 | Loss: 23.66227912902832

  0%|▋                                                                                                                                 | 23/4605 [02:34<6:19:40,  4.97s/it]
Epoch: 0 | Step: 23 | Loss: 16.258968353271484
Epoch: 0 | Step: 23 | Loss: 16.909114837646484

  1%|▋                                                                                                                                 | 24/4605 [02:39<6:19:29,  4.97s/it]
Epoch: 0 | Step: 23 | Loss: 19.525291442871094
Epoch: 0 | Step: 24 | Loss: 18.22751808166504
Epoch: 0 | Step: 24 | Loss: 18.965179443359375
Epoch: 0 | Step: 24 | Loss: 18.78595733642578

  1%|▋                                                                                                                                 | 25/4605 [02:44<6:19:25,  4.97s/it]
Epoch: 0 | Step: 25 | Loss: 17.816509246826172
Epoch: 0 | Step: 25 | Loss: 17.503618240356445

  1%|▋                                                                                                                                 | 26/4605 [02:49<6:19:21,  4.97s/it]
Epoch: 0 | Step: 25 | Loss: 20.274574279785156
Epoch: 0 | Step: 26 | Loss: 20.613988876342773
Epoch: 0 | Step: 26 | Loss: 20.138919830322266
Epoch: 0 | Step: 26 | Loss: 15.5425443649292

  1%|▊                                                                                                                                 | 27/4605 [02:54<6:19:19,  4.97s/it]
Epoch: 0 | Step: 27 | Loss: 16.07881736755371
Epoch: 0 | Step: 27 | Loss: 21.61097526550293

  1%|▊                                                                                                                                 | 28/4605 [02:59<6:19:19,  4.97s/it]
Epoch: 0 | Step: 27 | Loss: 17.84589385986328
Epoch: 0 | Step: 28 | Loss: 19.583175659179688
Epoch: 0 | Step: 28 | Loss: 17.475467681884766

  1%|▊                                                                                                                                 | 29/4605 [03:04<6:19:14,  4.97s/it]
Epoch: 0 | Step: 28 | Loss: 18.741365432739258
Epoch: 0 | Step: 29 | Loss: 21.236539840698242
Epoch: 0 | Step: 29 | Loss: 20.109146118164062

  1%|▊                                                                                                                                 | 30/4605 [03:09<6:19:16,  4.97s/it]
Epoch: 0 | Step: 29 | Loss: 20.472665786743164
Epoch: 0 | Step: 30 | Loss: 18.994638442993164
Epoch: 0 | Step: 30 | Loss: 17.777664184570312
Epoch: 0 | Step: 30 | Loss: 20.977447509765625

  1%|▉                                                                                                                                 | 31/4605 [03:14<6:19:06,  4.97s/it]
Epoch: 0 | Step: 31 | Loss: 18.51152801513672
Epoch: 0 | Step: 31 | Loss: 19.390979766845703

  1%|▉                                                                                                                                 | 32/4605 [03:19<6:19:06,  4.97s/it]
Epoch: 0 | Step: 31 | Loss: 19.358915328979492
Epoch: 0 | Step: 32 | Loss: 18.237842559814453
Epoch: 0 | Step: 32 | Loss: 16.634428024291992
Epoch: 0 | Step: 32 | Loss: 20.536142349243164

  1%|▉                                                                                                                                 | 33/4605 [03:24<6:19:06,  4.98s/it]
Epoch: 0 | Step: 33 | Loss: 18.7385196685791
Epoch: 0 | Step: 33 | Loss: 14.811092376708984
Epoch: 0 | Step: 33 | Loss: 20.331331253051758

  1%|▉                                                                                                                                 | 34/4605 [03:29<6:18:58,  4.97s/it]
Epoch: 0 | Step: 34 | Loss: 15.72330379486084
Epoch: 0 | Step: 34 | Loss: 19.30091094970703

  1%|▉                                                                                                                                 | 35/4605 [03:34<6:18:54,  4.97s/it]
Epoch: 0 | Step: 34 | Loss: 15.088576316833496
Epoch: 0 | Step: 35 | Loss: 20.494352340698242
Epoch: 0 | Step: 35 | Loss: 19.07223129272461
Epoch: 0 | Step: 35 | Loss: 16.151622772216797

  1%|█                                                                                                                                 | 36/4605 [03:38<6:18:53,  4.98s/it]
Epoch: 0 | Step: 36 | Loss: 17.060007095336914
Epoch: 0 | Step: 36 | Loss: 18.438791275024414
  1%|█                                                                                                                                 | 36/4605 [03:38<6:18:53,  4.98s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 959, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 752, in main
    accelerator.backward(loss)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt