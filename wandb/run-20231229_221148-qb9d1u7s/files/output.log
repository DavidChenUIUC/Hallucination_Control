12/29/2023 22:11:48 - INFO - __main__ - ***** Running training *****
12/29/2023 22:11:48 - INFO - __main__ -   Num examples = 14732
12/29/2023 22:11:48 - INFO - __main__ -   Num Epochs = 1
12/29/2023 22:11:48 - INFO - __main__ -   Instantaneous batch size per device = 2
12/29/2023 22:11:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/29/2023 22:11:48 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 22:11:48 - INFO - __main__ -   Total optimization steps = 1842
  0%|                                                                                                                                             | 0/1842 [00:00<?, ?it/s]12/29/2023 22:11:49 - INFO - accelerate.accelerator - Saving current state to exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0
12/29/2023 22:11:55 - INFO - accelerate.checkpointing - Model weights saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/model.safetensors
12/29/2023 22:11:55 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/optimizer.bin
12/29/2023 22:11:55 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/scheduler.bin
12/29/2023 22:11:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/sampler.bin
12/29/2023 22:11:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/sampler_1.bin
12/29/2023 22:11:55 - INFO - accelerate.checkpointing - Random states saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/random_states_0.pkl
tokenizer config file saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/tokenizer_config.json
Special tokens file saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/special_tokens_map.json
12/29/2023 22:11:56 - INFO - accelerate.accelerator - Saving current state to exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0
12/29/2023 22:12:06 - INFO - accelerate.checkpointing - Model weights saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/model.safetensors
12/29/2023 22:12:06 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/optimizer.bin
12/29/2023 22:12:06 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/scheduler.bin
12/29/2023 22:12:06 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/sampler.bin
12/29/2023 22:12:06 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/sampler_1.bin
12/29/2023 22:12:06 - INFO - accelerate.checkpointing - Random states saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/random_states_0.pkl
tokenizer config file saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/tokenizer_config.json
Special tokens file saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/special_tokens_map.json
12/29/2023 22:12:12 - INFO - accelerate.accelerator - Saving current state to exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0
12/29/2023 22:12:19 - INFO - accelerate.checkpointing - Model weights saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/model.safetensors
12/29/2023 22:12:19 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/optimizer.bin
12/29/2023 22:12:19 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/scheduler.bin
12/29/2023 22:12:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/sampler.bin
12/29/2023 22:12:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/sampler_1.bin
12/29/2023 22:12:19 - INFO - accelerate.checkpointing - Random states saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/step_0/random_states_0.pkl
tokenizer config file saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/tokenizer_config.json
Special tokens file saved in exp_results/prompted_bs2_len100_lr1e-4_decay0.1/special_tokens_map.json
  0%|                                                                                                                                  | 1/1842 [00:32<16:28:11, 32.21s/it]
Epoch: 0 | Step: 1 | Loss: 20.98577308654785
Epoch: 0 | Step: 1 | Loss: 21.557580947875977

  0%|▏                                                                                                                                  | 2/1842 [00:34<7:26:16, 14.55s/it]
Epoch: 0 | Step: 1 | Loss: 19.36200523376465
Epoch: 0 | Step: 2 | Loss: 21.4399471282959
Epoch: 0 | Step: 2 | Loss: 21.058189392089844
Epoch: 0 | Step: 2 | Loss: 18.159421920776367

  0%|▎                                                                                                                                  | 4/1842 [00:37<2:59:44,  5.87s/it]
Epoch: 0 | Step: 3 | Loss: 20.11821174621582
Epoch: 0 | Step: 3 | Loss: 21.054702758789062
Epoch: 0 | Step: 3 | Loss: 21.12105369567871
Epoch: 0 | Step: 3 | Loss: 19.59496307373047
  0%|▎                                                                                                                                  | 4/1842 [00:37<2:59:44,  5.87s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 959, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 752, in main
    accelerator.backward(loss)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Epoch: 0 | Step: 4 | Loss: 20.424652099609375
Epoch: 0 | Step: 4 | Loss: 20.52355194091797