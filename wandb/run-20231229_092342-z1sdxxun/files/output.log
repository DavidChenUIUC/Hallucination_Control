12/29/2023 09:23:44 - INFO - __main__ - ***** Running training *****
12/29/2023 09:23:44 - INFO - __main__ -   Num examples = 14732
12/29/2023 09:23:44 - INFO - __main__ -   Num Epochs = 5
12/29/2023 09:23:44 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 09:23:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 09:23:44 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 09:23:44 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                                                                                   | 0/4605 [00:00<?, ?it/s]12/29/2023 09:23:45 - INFO - accelerate.accelerator - Saving current state to exp_results/high_lr_linear_decay/step_0
12/29/2023 09:24:01 - INFO - accelerate.checkpointing - Model weights saved in exp_results/high_lr_linear_decay/step_0/model.safetensors
12/29/2023 09:24:01 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 09:24:01 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 09:24:01 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/high_lr_linear_decay/step_0/sampler.bin
12/29/2023 09:24:01 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 09:24:01 - INFO - accelerate.checkpointing - Random states saved in exp_results/high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 09:24:02 - INFO - accelerate.accelerator - Saving current state to exp_results/high_lr_linear_decay/step_0
12/29/2023 09:24:17 - INFO - accelerate.checkpointing - Model weights saved in exp_results/high_lr_linear_decay/step_0/model.safetensors
12/29/2023 09:24:17 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 09:24:17 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 09:24:17 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/high_lr_linear_decay/step_0/sampler.bin
12/29/2023 09:24:17 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 09:24:17 - INFO - accelerate.checkpointing - Random states saved in exp_results/high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 09:24:19 - INFO - accelerate.accelerator - Saving current state to exp_results/high_lr_linear_decay/step_0
12/29/2023 09:24:26 - INFO - accelerate.checkpointing - Model weights saved in exp_results/high_lr_linear_decay/step_0/model.safetensors
12/29/2023 09:24:26 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 09:24:26 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 09:24:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/high_lr_linear_decay/step_0/sampler.bin
12/29/2023 09:24:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 09:24:26 - INFO - accelerate.checkpointing - Random states saved in exp_results/high_lr_linear_decay/step_0/random_states_0.pkl
  0%|                                                                                                                                                                                        | 1/4605 [00:43<55:57:44, 43.76s/it]
Epoch: 0 | Step: 1 | Loss: 7.935431957244873
Epoch: 0 | Step: 1 | Loss: 6.915266990661621

  0%|                                                                                                                                                                                        | 2/4605 [00:48<26:43:02, 20.90s/it]
Epoch: 0 | Step: 1 | Loss: 8.899575233459473
Epoch: 0 | Step: 2 | Loss: 4.965171813964844
Epoch: 0 | Step: 2 | Loss: 8.449228286743164

  0%|                                                                                                                                                                                        | 3/4605 [00:53<17:22:38, 13.59s/it]
Epoch: 0 | Step: 2 | Loss: 9.526229858398438
Epoch: 0 | Step: 3 | Loss: 2.4394755363464355
  0%|                                                                                                                                                                                        | 3/4605 [00:53<17:22:38, 13.59s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 909, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 714, in main
    outputs = model(**batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1073, in forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 100, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 810, in forward
    hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 268, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 313, in forward
    output = output.to(expected_dtype)
KeyboardInterrupt
Epoch: 0 | Step: 3 | Loss: 7.186334609985352