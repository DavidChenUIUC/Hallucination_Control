12/29/2023 16:55:55 - INFO - __main__ - ***** Running training *****
12/29/2023 16:55:55 - INFO - __main__ -   Num examples = 14732
12/29/2023 16:55:55 - INFO - __main__ -   Num Epochs = 5
12/29/2023 16:55:55 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 16:55:55 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 16:55:55 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 16:55:55 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                                                                                  | 0/4605 [00:00<?, ?it/s]12/29/2023 16:55:56 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_low_lr_cosine_warmup/step_0
12/29/2023 16:56:04 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/model.safetensors
12/29/2023 16:56:04 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/optimizer.bin
12/29/2023 16:56:04 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/scheduler.bin
12/29/2023 16:56:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/sampler.bin
12/29/2023 16:56:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/sampler_1.bin
12/29/2023 16:56:04 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/random_states_0.pkl
12/29/2023 16:56:05 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_low_lr_cosine_warmup/step_0
12/29/2023 16:56:13 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/model.safetensors
12/29/2023 16:56:13 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/optimizer.bin
12/29/2023 16:56:13 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/scheduler.bin
12/29/2023 16:56:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/sampler.bin
12/29/2023 16:56:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/sampler_1.bin
12/29/2023 16:56:13 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/random_states_0.pkl
12/29/2023 16:56:15 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_low_lr_cosine_warmup/step_0
12/29/2023 16:56:22 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/model.safetensors
12/29/2023 16:56:22 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/optimizer.bin
12/29/2023 16:56:22 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/scheduler.bin
12/29/2023 16:56:22 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/sampler.bin
12/29/2023 16:56:22 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/sampler_1.bin
12/29/2023 16:56:22 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_low_lr_cosine_warmup/step_0/random_states_0.pkl
  0%|                                                                                                                                                                                       | 1/4605 [00:28<36:24:08, 28.46s/it]
Epoch: 0 | Step: 1 | Loss: 11.468790054321289
Epoch: 0 | Step: 1 | Loss: 11.466708183288574

  0%|                                                                                                                                                                                       | 2/4605 [00:33<19:04:43, 14.92s/it]
Epoch: 0 | Step: 1 | Loss: 11.470924377441406
Epoch: 0 | Step: 2 | Loss: 11.468000411987305
Epoch: 0 | Step: 2 | Loss: 11.464993476867676

  0%|                                                                                                                                                                                       | 3/4605 [00:38<13:10:59, 10.31s/it]
Epoch: 0 | Step: 2 | Loss: 11.466242790222168
Epoch: 0 | Step: 3 | Loss: 11.464418411254883

  0%|▏                                                                                                                                                                                      | 4/4605 [00:43<10:24:48,  8.15s/it]
Epoch: 0 | Step: 3 | Loss: 11.46464729309082
Epoch: 0 | Step: 3 | Loss: 11.462692260742188
Epoch: 0 | Step: 4 | Loss: 11.460474967956543
Epoch: 0 | Step: 4 | Loss: 11.461141586303711

  0%|▏                                                                                                                                                                                       | 5/4605 [00:48<8:53:15,  6.96s/it]
Epoch: 0 | Step: 4 | Loss: 11.464865684509277
Epoch: 0 | Step: 5 | Loss: 11.457733154296875
Epoch: 0 | Step: 5 | Loss: 11.460524559020996

  0%|▏                                                                                                                                                                                       | 6/4605 [00:53<7:58:29,  6.24s/it]
Epoch: 0 | Step: 5 | Loss: 11.45869255065918
Epoch: 0 | Step: 6 | Loss: 11.455737113952637
Epoch: 0 | Step: 6 | Loss: 11.452723503112793
Epoch: 0 | Step: 6 | Loss: 11.452594757080078

  0%|▎                                                                                                                                                                                       | 7/4605 [00:58<7:23:59,  5.79s/it]
Epoch: 0 | Step: 7 | Loss: 11.448230743408203
Epoch: 0 | Step: 7 | Loss: 11.447571754455566

  0%|▎                                                                                                                                                                                       | 8/4605 [01:02<7:01:13,  5.50s/it]
Epoch: 0 | Step: 7 | Loss: 11.445683479309082
Epoch: 0 | Step: 8 | Loss: 11.444507598876953

  0%|▎                                                                                                                                                                                       | 9/4605 [01:07<6:46:03,  5.30s/it]
Epoch: 0 | Step: 8 | Loss: 11.441370964050293
Epoch: 0 | Step: 8 | Loss: 11.44084358215332
Epoch: 0 | Step: 9 | Loss: 11.433233261108398
Epoch: 0 | Step: 9 | Loss: 11.431023597717285

  0%|▍                                                                                                                                                                                      | 10/4605 [01:12<6:35:45,  5.17s/it]
Epoch: 0 | Step: 9 | Loss: 11.433765411376953
Epoch: 0 | Step: 10 | Loss: 11.428821563720703

  0%|▍                                                                                                                                                                                      | 11/4605 [01:17<6:28:48,  5.08s/it]
Epoch: 0 | Step: 10 | Loss: 11.42744255065918
Epoch: 0 | Step: 10 | Loss: 11.428779602050781
Epoch: 0 | Step: 11 | Loss: 11.415295600891113
Epoch: 0 | Step: 11 | Loss: 11.418133735656738

  0%|▍                                                                                                                                                                                      | 12/4605 [01:22<6:24:07,  5.02s/it]
Epoch: 0 | Step: 11 | Loss: 11.416657447814941
Epoch: 0 | Step: 12 | Loss: 11.410253524780273
Epoch: 0 | Step: 12 | Loss: 11.405407905578613

  0%|▌                                                                                                                                                                                      | 13/4605 [01:27<6:21:01,  4.98s/it]
Epoch: 0 | Step: 12 | Loss: 11.406885147094727
Epoch: 0 | Step: 13 | Loss: 11.395758628845215

  0%|▌                                                                                                                                                                                      | 14/4605 [01:32<6:18:55,  4.95s/it]
Epoch: 0 | Step: 13 | Loss: 11.398844718933105
Epoch: 0 | Step: 13 | Loss: 11.393287658691406
Epoch: 0 | Step: 14 | Loss: 11.38554859161377
Epoch: 0 | Step: 14 | Loss: 11.385599136352539

  0%|▌                                                                                                                                                                                      | 15/4605 [01:37<6:17:21,  4.93s/it]
Epoch: 0 | Step: 14 | Loss: 11.38606071472168
Epoch: 0 | Step: 15 | Loss: 11.374072074890137

  0%|▋                                                                                                                                                                                      | 16/4605 [01:42<6:16:15,  4.92s/it]
Epoch: 0 | Step: 15 | Loss: 11.371323585510254
Epoch: 0 | Step: 15 | Loss: 11.373356819152832
Epoch: 0 | Step: 16 | Loss: 11.35952091217041
Epoch: 0 | Step: 16 | Loss: 11.357586860656738

  0%|▋                                                                                                                                                                                      | 17/4605 [01:46<6:15:28,  4.91s/it]
Epoch: 0 | Step: 16 | Loss: 11.359818458557129
Epoch: 0 | Step: 17 | Loss: 11.350152015686035

  0%|▋                                                                                                                                                                                      | 18/4605 [01:51<6:14:58,  4.90s/it]
Epoch: 0 | Step: 17 | Loss: 11.34841537475586
Epoch: 0 | Step: 17 | Loss: 11.348118782043457
Epoch: 0 | Step: 18 | Loss: 11.330374717712402
Epoch: 0 | Step: 18 | Loss: 11.332338333129883

  0%|▊                                                                                                                                                                                      | 19/4605 [01:56<6:14:36,  4.90s/it]
Epoch: 0 | Step: 18 | Loss: 11.331547737121582
Epoch: 0 | Step: 19 | Loss: 11.315776824951172
Epoch: 0 | Step: 19 | Loss: 11.320061683654785

  0%|▊                                                                                                                                                                                      | 20/4605 [02:01<6:14:19,  4.90s/it]
Epoch: 0 | Step: 19 | Loss: 11.320425987243652
Epoch: 0 | Step: 20 | Loss: 11.304022789001465
Epoch: 0 | Step: 20 | Loss: 11.300640106201172

  0%|▊                                                                                                                                                                                      | 21/4605 [02:06<6:14:05,  4.90s/it]
Epoch: 0 | Step: 20 | Loss: 11.300419807434082
Epoch: 0 | Step: 21 | Loss: 11.286755561828613
Epoch: 0 | Step: 21 | Loss: 11.28561019897461

  0%|▊                                                                                                                                                                                      | 22/4605 [02:11<6:14:00,  4.90s/it]
Epoch: 0 | Step: 21 | Loss: 11.287483215332031
Epoch: 0 | Step: 22 | Loss: 11.268291473388672

  0%|▉                                                                                                                                                                                      | 23/4605 [02:16<6:13:58,  4.90s/it]
Epoch: 0 | Step: 22 | Loss: 11.267590522766113
Epoch: 0 | Step: 22 | Loss: 11.270983695983887
Epoch: 0 | Step: 23 | Loss: 11.251516342163086
Epoch: 0 | Step: 23 | Loss: 11.251335144042969

  1%|▉                                                                                                                                                                                      | 24/4605 [02:21<6:13:56,  4.90s/it]
Epoch: 0 | Step: 23 | Loss: 11.249506950378418
Epoch: 0 | Step: 24 | Loss: 11.234179496765137

  1%|▉                                                                                                                                                                                      | 25/4605 [02:26<6:13:48,  4.90s/it]
Epoch: 0 | Step: 24 | Loss: 11.232142448425293
Epoch: 0 | Step: 24 | Loss: 11.23517894744873
Epoch: 0 | Step: 25 | Loss: 11.22011947631836
Epoch: 0 | Step: 25 | Loss: 11.219538688659668

  1%|█                                                                                                                                                                                      | 26/4605 [02:30<6:13:49,  4.90s/it]
Epoch: 0 | Step: 25 | Loss: 11.219074249267578
Epoch: 0 | Step: 26 | Loss: 11.197892189025879

  1%|█                                                                                                                                                                                      | 27/4605 [02:35<6:13:47,  4.90s/it]
Epoch: 0 | Step: 26 | Loss: 11.199731826782227
Epoch: 0 | Step: 26 | Loss: 11.197294235229492
Epoch: 0 | Step: 27 | Loss: 11.181716918945312
Epoch: 0 | Step: 27 | Loss: 11.179577827453613

  1%|█                                                                                                                                                                                      | 28/4605 [02:40<6:13:47,  4.90s/it]
Epoch: 0 | Step: 27 | Loss: 11.176241874694824
Epoch: 0 | Step: 28 | Loss: 11.158231735229492
Epoch: 0 | Step: 28 | Loss: 11.159000396728516

  1%|█▏                                                                                                                                                                                     | 29/4605 [02:45<6:13:49,  4.90s/it]
Epoch: 0 | Step: 28 | Loss: 11.160788536071777
Epoch: 0 | Step: 29 | Loss: 11.140165328979492
Epoch: 0 | Step: 29 | Loss: 11.13942813873291

  1%|█▏                                                                                                                                                                                     | 30/4605 [02:50<6:13:54,  4.90s/it]
Epoch: 0 | Step: 29 | Loss: 11.141923904418945
Epoch: 0 | Step: 30 | Loss: 11.119904518127441
Epoch: 0 | Step: 30 | Loss: 11.1206693649292

  1%|█▏                                                                                                                                                                                     | 31/4605 [02:55<6:13:47,  4.90s/it]
Epoch: 0 | Step: 30 | Loss: 11.120463371276855
Epoch: 0 | Step: 31 | Loss: 11.099936485290527
Epoch: 0 | Step: 31 | Loss: 11.101298332214355
Epoch: 0 | Step: 31 | Loss: 11.100220680236816

  1%|█▎                                                                                                                                                                                     | 32/4605 [03:00<6:13:40,  4.90s/it]
Epoch: 0 | Step: 32 | Loss: 11.081441879272461
Epoch: 0 | Step: 32 | Loss: 11.081241607666016

  1%|█▎                                                                                                                                                                                     | 33/4605 [03:05<6:13:35,  4.90s/it]
Epoch: 0 | Step: 32 | Loss: 11.07923412322998
Epoch: 0 | Step: 33 | Loss: 11.06140422821045

  1%|█▎                                                                                                                                                                                     | 34/4605 [03:10<6:13:33,  4.90s/it]
Epoch: 0 | Step: 33 | Loss: 11.058788299560547
Epoch: 0 | Step: 33 | Loss: 11.061298370361328
Epoch: 0 | Step: 34 | Loss: 11.040112495422363
Epoch: 0 | Step: 34 | Loss: 11.037812232971191

  1%|█▍                                                                                                                                                                                     | 35/4605 [03:15<6:13:25,  4.90s/it]
Epoch: 0 | Step: 34 | Loss: 11.03630542755127
Epoch: 0 | Step: 35 | Loss: 11.015142440795898

  1%|█▍                                                                                                                                                                                     | 36/4605 [03:20<6:13:22,  4.90s/it]
Epoch: 0 | Step: 35 | Loss: 11.019165992736816
Epoch: 0 | Step: 35 | Loss: 11.016602516174316
Epoch: 0 | Step: 36 | Loss: 10.995064735412598
Epoch: 0 | Step: 36 | Loss: 10.99616527557373

  1%|█▍                                                                                                                                                                                     | 37/4605 [03:24<6:13:12,  4.90s/it]
Epoch: 0 | Step: 36 | Loss: 10.993699073791504
Epoch: 0 | Step: 37 | Loss: 10.971261978149414

  1%|█▌                                                                                                                                                                                     | 38/4605 [03:29<6:13:10,  4.90s/it]
Epoch: 0 | Step: 37 | Loss: 10.973837852478027
Epoch: 0 | Step: 37 | Loss: 10.972573280334473
Epoch: 0 | Step: 38 | Loss: 10.95200252532959
Epoch: 0 | Step: 38 | Loss: 10.951380729675293

  1%|█▌                                                                                                                                                                                     | 39/4605 [03:34<6:13:02,  4.90s/it]
Epoch: 0 | Step: 38 | Loss: 10.949332237243652
Epoch: 0 | Step: 39 | Loss: 10.927976608276367
Epoch: 0 | Step: 39 | Loss: 10.9269380569458

  1%|█▌                                                                                                                                                                                     | 40/4605 [03:39<6:12:59,  4.90s/it]
Epoch: 0 | Step: 39 | Loss: 10.927809715270996
Epoch: 0 | Step: 40 | Loss: 10.907886505126953
Epoch: 0 | Step: 40 | Loss: 10.904497146606445
Epoch: 0 | Step: 40 | Loss: 10.905855178833008

  1%|█▋                                                                                                                                                                                     | 41/4605 [03:44<6:12:50,  4.90s/it]
Epoch: 0 | Step: 41 | Loss: 10.8822660446167
Epoch: 0 | Step: 41 | Loss: 10.883198738098145

  1%|█▋                                                                                                                                                                                     | 42/4605 [03:49<6:12:45,  4.90s/it]
Epoch: 0 | Step: 41 | Loss: 10.88208293914795
Epoch: 0 | Step: 42 | Loss: 10.857526779174805

  1%|█▋                                                                                                                                                                                     | 43/4605 [03:54<6:12:37,  4.90s/it]
Epoch: 0 | Step: 42 | Loss: 10.85549545288086
Epoch: 0 | Step: 42 | Loss: 10.860414505004883
Epoch: 0 | Step: 43 | Loss: 10.83581829071045
Epoch: 0 | Step: 43 | Loss: 10.837482452392578

  1%|█▋                                                                                                                                                                                     | 44/4605 [03:59<6:12:39,  4.90s/it]
Epoch: 0 | Step: 43 | Loss: 10.835774421691895
Epoch: 0 | Step: 44 | Loss: 10.81011962890625

  1%|█▊                                                                                                                                                                                     | 45/4605 [04:04<6:12:31,  4.90s/it]
Epoch: 0 | Step: 44 | Loss: 10.811537742614746
Epoch: 0 | Step: 44 | Loss: 10.809870719909668
Epoch: 0 | Step: 45 | Loss: 10.786800384521484
Epoch: 0 | Step: 45 | Loss: 10.788714408874512

  1%|█▊                                                                                                                                                                                     | 46/4605 [04:09<6:12:23,  4.90s/it]
Epoch: 0 | Step: 45 | Loss: 10.786511421203613
Epoch: 0 | Step: 46 | Loss: 10.761129379272461

  1%|█▊                                                                                                                                                                                     | 47/4605 [04:13<6:12:17,  4.90s/it]
Epoch: 0 | Step: 46 | Loss: 10.75804615020752
Epoch: 0 | Step: 46 | Loss: 10.760906219482422
Epoch: 0 | Step: 47 | Loss: 10.734953880310059
Epoch: 0 | Step: 47 | Loss: 10.734070777893066

  1%|█▉                                                                                                                                                                                     | 48/4605 [04:18<6:12:16,  4.90s/it]
Epoch: 0 | Step: 47 | Loss: 10.734365463256836
Epoch: 0 | Step: 48 | Loss: 10.708818435668945
Epoch: 0 | Step: 48 | Loss: 10.708867073059082

  1%|█▉                                                                                                                                                                                     | 49/4605 [04:23<6:12:10,  4.90s/it]
Epoch: 0 | Step: 48 | Loss: 10.70884895324707
Epoch: 0 | Step: 49 | Loss: 10.68016242980957
Epoch: 0 | Step: 49 | Loss: 10.681586265563965

  1%|█▉                                                                                                                                                                                     | 50/4605 [04:28<6:12:06,  4.90s/it]

  1%|██                                                                                                                                                                                     | 51/4605 [04:33<6:11:13,  4.89s/it]
Epoch: 0 | Step: 51 | Loss: 10.626837730407715

  1%|██                                                                                                                                                                                     | 52/4605 [04:38<6:12:13,  4.91s/it]
Epoch: 0 | Step: 51 | Loss: 10.625903129577637
Epoch: 0 | Step: 51 | Loss: 10.623566627502441
Epoch: 0 | Step: 52 | Loss: 10.596477508544922
Epoch: 0 | Step: 52 | Loss: 10.59864330291748

  1%|██                                                                                                                                                                                     | 53/4605 [04:43<6:12:13,  4.91s/it]
Epoch: 0 | Step: 52 | Loss: 10.598130226135254
Epoch: 0 | Step: 53 | Loss: 10.567304611206055

  1%|██▏                                                                                                                                                                                    | 54/4605 [04:48<6:12:07,  4.91s/it]
Epoch: 0 | Step: 53 | Loss: 10.570033073425293
Epoch: 0 | Step: 53 | Loss: 10.5707426071167
Epoch: 0 | Step: 54 | Loss: 10.540075302124023
Epoch: 0 | Step: 54 | Loss: 10.538789749145508

  1%|██▏                                                                                                                                                                                    | 55/4605 [04:53<6:12:02,  4.91s/it]
Epoch: 0 | Step: 54 | Loss: 10.541070938110352
Epoch: 0 | Step: 55 | Loss: 10.511063575744629

  1%|██▏                                                                                                                                                                                    | 56/4605 [04:58<6:11:58,  4.91s/it]
Epoch: 0 | Step: 55 | Loss: 10.508562088012695
  1%|██▏                                                                                                                                                                                    | 56/4605 [04:58<6:11:58,  4.91s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 916, in <module>
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 714, in main
    for step, batch in enumerate(active_dataloader):
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1073, in forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 100, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 796, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 388, in forward
    value_states = self.v_proj(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 311, in forward
    output = lora_B(lora_A(dropout(x)))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Epoch: 0 | Step: 56 | Loss: 10.4782133102417