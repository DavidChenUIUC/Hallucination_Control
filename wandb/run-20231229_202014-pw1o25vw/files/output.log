12/29/2023 20:20:14 - INFO - __main__ - ***** Running training *****
12/29/2023 20:20:14 - INFO - __main__ -   Num examples = 14732
12/29/2023 20:20:14 - INFO - __main__ -   Num Epochs = 5
12/29/2023 20:20:14 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 20:20:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 20:20:14 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 20:20:14 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 20:20:14 - INFO - accelerate.accelerator - Loading states from ./exp_results/with_prompt_low_lr_linear_decay_5e-6/step_900
12/29/2023 20:20:15 - INFO - accelerate.checkpointing - All model weights loaded successfully
|- Resuming from checkpoint
Resumed from checkpoint: ./exp_results/with_prompt_low_lr_linear_decay_5e-6/step_900 path step_900
12/29/2023 20:20:16 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
12/29/2023 20:20:16 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
12/29/2023 20:20:16 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
12/29/2023 20:20:16 - INFO - accelerate.checkpointing - All random states loaded successfully
12/29/2023 20:20:17 - INFO - accelerate.accelerator - Loading in 0 custom states
 20%|█████████████████████████▍                                                                                                        | 900/4605 [00:02<00:09, 375.12it/s]




Traceback (most recent call last):██████▌                                                                                                 | 40/205 [02:26<10:02,  3.65s/it]
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 928, in <module>
    rouge_scores = {key: value * 100 for key, value in rouge_results.items()}
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 782, in main
    eval_on_every = 10
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1130, in generate
    outputs = self.base_model.generate(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1764, in generate
    return self.sample(
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2861, in sample
    outputs = self(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 810, in forward
    hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 268, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 290, in forward
    result = self.base_layer(x, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 256, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 577, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/functional.py", line 1041, in dequantize_4bit
    lib.cdequantize_blockwise_fp16_nf4(get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int(quant_state.blocksize), ct.c_int(n))
KeyboardInterrupt