12/29/2023 17:25:13 - INFO - __main__ - ***** Running training *****
12/29/2023 17:25:13 - INFO - __main__ -   Num examples = 14732
12/29/2023 17:25:13 - INFO - __main__ -   Num Epochs = 5
12/29/2023 17:25:13 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 17:25:13 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 17:25:13 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 17:25:13 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 17:25:14 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0
12/29/2023 17:25:21 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/model.safetensors
12/29/2023 17:25:21 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/optimizer.bin
12/29/2023 17:25:21 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/scheduler.bin
12/29/2023 17:25:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/sampler.bin
12/29/2023 17:25:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/sampler_1.bin
12/29/2023 17:25:21 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/random_states_0.pkl
12/29/2023 17:25:23 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0
12/29/2023 17:25:39 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/model.safetensors
12/29/2023 17:25:40 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/optimizer.bin
12/29/2023 17:25:40 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/scheduler.bin
12/29/2023 17:25:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/sampler.bin
12/29/2023 17:25:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/sampler_1.bin
12/29/2023 17:25:40 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/random_states_0.pkl
12/29/2023 17:25:42 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0
12/29/2023 17:26:00 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/model.safetensors
12/29/2023 17:26:00 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/optimizer.bin
12/29/2023 17:26:00 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/scheduler.bin
12/29/2023 17:26:00 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/sampler.bin
12/29/2023 17:26:00 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/sampler_1.bin
12/29/2023 17:26:00 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_low_lr_linear_decay_5e-6/step_0/random_states_0.pkl
  0%|                                                                                                                                  | 1/4605 [00:48<62:04:28, 48.54s/it]
Epoch: 0 | Step: 1 | Loss: 7.858945846557617
Epoch: 0 | Step: 1 | Loss: 5.856793403625488

  0%|                                                                                                                                  | 2/4605 [00:53<29:13:41, 22.86s/it]
Epoch: 0 | Step: 1 | Loss: 8.721248626708984
Epoch: 0 | Step: 2 | Loss: 4.859217643737793
Epoch: 0 | Step: 2 | Loss: 8.763995170593262
Epoch: 0 | Step: 2 | Loss: 10.829150199890137

  0%|                                                                                                                                  | 3/4605 [00:58<18:43:59, 14.65s/it]
Epoch: 0 | Step: 3 | Loss: 2.489276170730591
Epoch: 0 | Step: 3 | Loss: 7.310537815093994
Epoch: 0 | Step: 3 | Loss: 6.8723297119140625

  0%|                                                                                                                                  | 4/4605 [01:03<13:48:18, 10.80s/it]
Epoch: 0 | Step: 4 | Loss: 8.465576171875
Epoch: 0 | Step: 4 | Loss: 4.002541542053223

  0%|▏                                                                                                                                 | 5/4605 [01:08<11:04:59,  8.67s/it]
Epoch: 0 | Step: 4 | Loss: 5.970253944396973
Epoch: 0 | Step: 5 | Loss: 8.393499374389648
Epoch: 0 | Step: 5 | Loss: 10.15782356262207
Epoch: 0 | Step: 5 | Loss: 5.377533912658691

  0%|▏                                                                                                                                  | 6/4605 [01:13<9:26:38,  7.39s/it]
Epoch: 0 | Step: 6 | Loss: 10.38640022277832
Epoch: 0 | Step: 6 | Loss: 10.236739158630371

  0%|▏                                                                                                                                  | 7/4605 [01:17<8:24:32,  6.58s/it]
Epoch: 0 | Step: 6 | Loss: 9.816288948059082
Epoch: 0 | Step: 7 | Loss: 9.29542350769043
Epoch: 0 | Step: 7 | Loss: 4.273028373718262
Epoch: 0 | Step: 7 | Loss: 10.597919464111328

  0%|▏                                                                                                                                  | 8/4605 [01:22<7:43:54,  6.05s/it]
Epoch: 0 | Step: 8 | Loss: 7.948585033416748
Epoch: 0 | Step: 8 | Loss: 7.970562934875488

  0%|▎                                                                                                                                  | 9/4605 [01:27<7:16:55,  5.70s/it]
Epoch: 0 | Step: 8 | Loss: 9.8829927444458
Epoch: 0 | Step: 9 | Loss: 7.9072442054748535
Epoch: 0 | Step: 9 | Loss: 5.626894950866699
Epoch: 0 | Step: 9 | Loss: 6.467885971069336

  0%|▎                                                                                                                                 | 10/4605 [01:32<6:58:29,  5.46s/it]
Epoch: 0 | Step: 10 | Loss: 8.256665229797363
Epoch: 0 | Step: 10 | Loss: 4.042352676391602

  0%|▎                                                                                                                                 | 11/4605 [01:37<6:45:57,  5.30s/it]
Epoch: 0 | Step: 10 | Loss: 6.923409938812256
Epoch: 0 | Step: 11 | Loss: 8.915885925292969
Epoch: 0 | Step: 11 | Loss: 7.306746959686279
Epoch: 0 | Step: 11 | Loss: 6.756222724914551

  0%|▎                                                                                                                                 | 12/4605 [01:42<6:37:21,  5.19s/it]
Epoch: 0 | Step: 12 | Loss: 9.255613327026367
Epoch: 0 | Step: 12 | Loss: 9.072511672973633

  0%|▎                                                                                                                                 | 13/4605 [01:47<6:31:47,  5.12s/it]
Epoch: 0 | Step: 12 | Loss: 4.788678169250488
Epoch: 0 | Step: 13 | Loss: 5.177011013031006
Epoch: 0 | Step: 13 | Loss: 6.574859142303467
Epoch: 0 | Step: 13 | Loss: 6.4242143630981445

  0%|▍                                                                                                                                 | 14/4605 [01:52<6:27:43,  5.07s/it]
Epoch: 0 | Step: 14 | Loss: 8.386418342590332
Epoch: 0 | Step: 14 | Loss: 4.42696475982666

  0%|▍                                                                                                                                 | 15/4605 [01:57<6:24:46,  5.03s/it]
Epoch: 0 | Step: 14 | Loss: 6.93208122253418
Epoch: 0 | Step: 15 | Loss: 8.463261604309082
Epoch: 0 | Step: 15 | Loss: 7.3087968826293945
  0%|▍                                                                                                                                 | 15/4605 [01:57<6:24:46,  5.03s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 918, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 716, in main
    outputs = model(**batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1073, in forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 100, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 793, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 116, in forward
    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
KeyboardInterrupt