12/29/2023 20:41:03 - INFO - __main__ - ***** Running training *****
12/29/2023 20:41:03 - INFO - __main__ -   Num examples = 14732
12/29/2023 20:41:03 - INFO - __main__ -   Num Epochs = 5
12/29/2023 20:41:03 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 20:41:03 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 20:41:03 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 20:41:03 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 20:41:03 - INFO - accelerate.accelerator - Loading states from ./exp_results/test/step_0
12/29/2023 20:41:04 - INFO - accelerate.checkpointing - All model weights loaded successfully
12/29/2023 20:41:04 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
12/29/2023 20:41:04 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
12/29/2023 20:41:04 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
12/29/2023 20:41:04 - INFO - accelerate.checkpointing - All random states loaded successfully
12/29/2023 20:41:04 - INFO - accelerate.accelerator - Loading in 0 custom states
  0%|                                                                                                                                             | 0/4605 [00:01<?, ?it/s]
Evaluating:   0%|                                                                                                                                  | 0/205 [00:00<?, ?it/s]
|- Resuming from checkpoint
Resumed from checkpoint: ./exp_results/test/step_0 path step_0

Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 82/205 [00:37<00:39,  3.13it/s]
|- generated_tokens  ["Your task is to write a summarization for the following dialogue: 'Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him üôÇ\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye' üôÇ\nYour task is to write a summarization for the following dialogue:\n'Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: I'd rather you texted him\nAmanda: Just text him üôÇ\nHannah: Urgh.. Alright\nAmanda: Bye bye' üôÇ\nThe key idea of this task is that the chatbot is able to summarize the contents of a text conversation. The dialogue can be thought of as a conversation between two people, and the chatbot is asked to summarize the conversation. To do this, the chatbot will need to understand the contents of the conversation, and be able to generate a summary that captures the main points of the conversation.\nOne way to approach this task is to use an existing conversation summarization tool such as Rasa NLU. Rasa NLU is a tool for building conversational", "Your task is to write a summarization for the following dialogue: 'Eric: MACHINE!\r\nRob: That's so gr8!\r\nEric: I know! And shows how Americans see Russian ;)\r\nRob: And it's really funny!\r\nEric: I know! I especially like the train part!\r\nRob: Hahaha! No one talks to the machine like that!\r\nEric: Is this his only stand-up?\r\nRob: Idk. I'll check.\r\nEric: Sure.\r\nRob: Turns out no! There are some of his stand-ups on youtube.\r\nEric: Gr8! I'll watch them now!\r\nRob: Me too!\r\nEric: MACHINE!\r\nRob: MACHINE!\r\nEric: TTYL?\r\nRob: Sure :)'  \r\n", 'Your task is to write a summarization for the following dialogue: \'Lenny: Babe, can you help me with something?\r\nBob: Sure, what\'s up?\r\nLenny: Which one should I pick?\r\nBob: Send me photos\r\nLenny:  <file_photo>\r\nLenny:  <file_photo>\r\nLenny:  <file_photo>\r\nBob: I like the first ones best\r\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\r\nBob: I have four black pairs :D :D\r\nLenny: yeah, but shouldn\'t I pick a different color?\r\nBob: what matters is what you\'ll give you the most outfit options\r\nLenny: So I guess I\'ll buy the first or the third pair then\r\nBob: Pick the best quality then\r\nLenny: ur right, thx\r\nBob: no prob :)\'  \r\n  \r\nThe first step is to convert the string to a list of strings. For this purpose, we need the string module which is imported with the following line:  \r\n  \r\nimport string \r\n\r\nThe following code will create a list of all letters of the alphabet (including space):  \r\n  \r\nalphabet = string.ascii_lowercase + " " \r\n\r\nThe following code will create a list of strings from the dialogue:  \r\n  \r\ndialogue = "Lenny: Babe, can you help me with something?\\nBob: Sure, what\'s up?\\nLenny: Which one should I pick?\\nBob: Send me photos\\nLenny:  <file_photo>\\nLenny:  <file_photo>\\nLenny:  <file_photo>\\nBob: I like the first ones best\\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\\nBob: I have four black pairs :D :D\\nLenny: yeah, but shouldn\'t I pick a different color?\\nBob: what matters is what', "Your task is to write a summarization for the following dialogue: 'Will: hey babe, what do you want for dinner tonight?\r\nEmma:  gah, don't even worry about it tonight\r\nWill: what do you mean? everything ok?\r\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\r\nWill: Well what time will you be home?\r\nEmma: soon, hopefully\r\nWill: you sure? Maybe you want me to pick you up?\r\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \r\nWill: Alright, love you. \r\nEmma: love you too. '  \r\nNote: This task is similar to Task 7 in the [Natural Language Processing course](https://www.coursera.org/learn/natural-language-processing), which is part of the [Machine Learning course](https://www.coursera.org/specializations/machine-learning).\r\n\r\n## Solution\r\n\r\nThe solution will be posted soon."]
|- single pred  ["üôÇ\nYour task is to write a summarization for the following dialogue:\n'Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: I'd rather you texted him\nAmanda: Just text him üôÇ\nHannah: Urgh.. Alright\nAmanda: Bye bye' üôÇ\nThe key idea of this task is that the chatbot is able to summarize the contents of a text conversation. The dialogue can be thought of as a conversation between two people, and the chatbot is asked to summarize the conversation. To do this, the chatbot will need to understand the contents of the conversation, and be able to generate a summary that captures the main points of the conversation.\nOne way to approach this task is to use an existing conversation summarization tool such as Rasa NLU. Rasa NLU is a tool for building conversational", '\r\n', '\r\n  \r\nThe first step is to convert the string to a list of strings. For this purpose, we need the string module which is imported with the following line:  \r\n  \r\nimport string \r\n\r\nThe following code will create a list of all letters of the alphabet (including space):  \r\n  \r\nalphabet = string.ascii_lowercase + " " \r\n\r\nThe following code will create a list of strings from the dialogue:  \r\n  \r\ndialogue = "Lenny: Babe, can you help me with something?\\nBob: Sure, what\'s up?\\nLenny: Which one should I pick?\\nBob: Send me photos\\nLenny:  <file_photo>\\nLenny:  <file_photo>\\nLenny:  <file_photo>\\nBob: I like the first ones best\\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\\nBob: I have four black pairs :D :D\\nLenny: yeah, but shouldn\'t I pick a different color?\\nBob: what matters is what', '\r\nNote: This task is similar to Task 7 in the [Natural Language Processing course](https://www.coursera.org/learn/natural-language-processing), which is part of the [Machine Learning course](https://www.coursera.org/specializations/machine-learning).\r\n\r\n## Solution\r\n\r\nThe solution will be posted soon.']
|- single label  ["Summrization: 'Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.' ", "Summrization: 'Eric and Rob are going to watch a stand-up on youtube.' ", "Summrization: 'Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.' ", "Summrization: 'Emma will be home soon and she will let Will know.' "]

Traceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 82/205 [00:48<00:39,  3.13it/s]
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 953, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 802, in main
    generated_tokens = accelerator.unwrap_model(model).generate(**gen_kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1130, in generate
    outputs = self.base_model.generate(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1764, in generate
    return self.sample(
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2861, in sample
    outputs = self(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 810, in forward
    hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 268, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 290, in forward
    result = self.base_layer(x, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 256, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 577, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
KeyboardInterrupt