12/29/2023 09:09:31 - INFO - __main__ - ***** Running training *****
12/29/2023 09:09:31 - INFO - __main__ -   Num examples = 14732
12/29/2023 09:09:31 - INFO - __main__ -   Num Epochs = 5
12/29/2023 09:09:31 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 09:09:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 09:09:31 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 09:09:31 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                                                                                   | 0/4605 [00:00<?, ?it/s]12/29/2023 09:09:32 - INFO - accelerate.accelerator - Saving current state to exp_results/high_lr_linear_decay/step_0
12/29/2023 09:09:38 - INFO - accelerate.checkpointing - Model weights saved in exp_results/high_lr_linear_decay/step_0/model.safetensors
12/29/2023 09:09:38 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 09:09:38 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 09:09:38 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/high_lr_linear_decay/step_0/sampler.bin
12/29/2023 09:09:38 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 09:09:38 - INFO - accelerate.checkpointing - Random states saved in exp_results/high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 09:10:14 - INFO - __main__ - epoch 0: accuracy: 0.5                                                                                                                                 | 1/205 [00:36<2:02:46, 36.11s/it]
12/29/2023 09:10:14 - INFO - absl - Using default tokenizer.
12/29/2023 09:10:14 - INFO - __main__ - epoch 0: ROUGE scores: {'rouge1': 24.710740124174173, 'rouge2': 4.074795943407232, 'rougeL': 13.794785417386478, 'rougeLsum': 13.794785417386478}
|- Printing evaluation prediction
[inf, 23.0, 1.0, inf]
|- Printing evaluation ground truth
[inf, 8.0, inf, inf]
12/29/2023 09:10:15 - INFO - accelerate.accelerator - Saving current state to exp_results/high_lr_linear_decay/step_0
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'accuracy': 0.5, 'rouge1': 24.710740124174173, 'rouge2': 4.074795943407232, 'rougeL': 13.794785417386478, 'epoch': 0, 'step': 0, '_timestamp': 1703841014.5308106}).
12/29/2023 09:10:25 - INFO - accelerate.checkpointing - Model weights saved in exp_results/high_lr_linear_decay/step_0/model.safetensors
12/29/2023 09:10:25 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 09:10:25 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 09:10:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/high_lr_linear_decay/step_0/sampler.bin
12/29/2023 09:10:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 09:10:25 - INFO - accelerate.checkpointing - Random states saved in exp_results/high_lr_linear_decay/step_0/random_states_0.pkl
Evaluating:   0%|                                                                                                                                                                                        | 0/205 [00:00<?, ?it/s]
|- Printing evaluation prediction
[6.0, 8.0, 4.0, inf]
|- Printing evaluation ground truth
12/29/2023 09:11:01 - INFO - __main__ - epoch 1: accuracy: 0.5                                                                                                                                 | 1/205 [00:36<2:03:05, 36.20s/it]
12/29/2023 09:11:01 - INFO - absl - Using default tokenizer.
12/29/2023 09:11:02 - INFO - __main__ - epoch 1: ROUGE scores: {'rouge1': 52.50792450787625, 'rouge2': 33.36453717628962, 'rougeL': 37.29773706948826, 'rougeLsum': 37.29773706948826}
12/29/2023 09:11:03 - INFO - accelerate.accelerator - Saving current state to exp_results/high_lr_linear_decay/step_0
12/29/2023 09:11:19 - INFO - accelerate.checkpointing - Model weights saved in exp_results/high_lr_linear_decay/step_0/model.safetensors
12/29/2023 09:11:19 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 09:11:19 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 09:11:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/high_lr_linear_decay/step_0/sampler.bin
12/29/2023 09:11:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 09:11:19 - INFO - accelerate.checkpointing - Random states saved in exp_results/high_lr_linear_decay/step_0/random_states_0.pkl
                                                                                                                                                                                                                                 [34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 3. Dropping entry: {'accuracy': 0.5, 'rouge1': 52.50792450787625, 'rouge2': 33.36453717628962, 'rougeL': 37.29773706948826, 'epoch': 1, 'step': 0, '_timestamp': 1703841062.0434952}).
Evaluating:   0%|                                                                                                                                                                                        | 0/205 [00:32<?, ?it/s]
Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 911, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 766, in main
    generated_tokens = accelerator.unwrap_model(model).generate(**gen_kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1130, in generate
    outputs = self.base_model.generate(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1764, in generate
    return self.sample(
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2861, in sample
    outputs = self(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 796, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 387, in forward
    key_states = self.k_proj(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 290, in forward
    result = self.base_layer(x, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 256, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
KeyboardInterrupt