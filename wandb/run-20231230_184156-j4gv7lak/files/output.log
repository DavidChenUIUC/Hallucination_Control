Train dataset size: 14732
Test dataset size: 819
Map: 100%|██████████████████████████████████████████████████████████████████████████| 15551/15551 [00:01<00:00, 13145.94 examples/s]
Max source length: 255
Max target length: 50
Map: 100%|██████████████████████████████████████████████████████████████████████████| 15551/15551 [00:00<00:00, 49891.78 examples/s]
Map:   0%|                                                                                         | 0/14732 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/flan_t5.py", line 253, in <module>
    main()
  File "/home/cwtang/david_dynamo/flan_t5.py", line 150, in main
    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["dialogue", "summary", "id"])
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3093, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/cwtang/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/cwtang/david_dynamo/flan_t5.py", line 46, in preprocess_function
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)
NameError: name 'tokenizer' is not defined