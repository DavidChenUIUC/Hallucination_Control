12/29/2023 16:59:54 - INFO - __main__ - ***** Running training *****
12/29/2023 16:59:54 - INFO - __main__ -   Num examples = 14732
12/29/2023 16:59:54 - INFO - __main__ -   Num Epochs = 5
12/29/2023 16:59:54 - INFO - __main__ -   Instantaneous batch size per device = 4
12/29/2023 16:59:54 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
12/29/2023 16:59:54 - INFO - __main__ -   Gradient Accumulation steps = 4
12/29/2023 16:59:54 - INFO - __main__ -   Total optimization steps = 4605
  0%|                                                                                                                                             | 0/4605 [00:00<?, ?it/s]12/29/2023 16:59:55 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_high_lr_linear_decay/step_0
12/29/2023 17:00:09 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_high_lr_linear_decay/step_0/model.safetensors
12/29/2023 17:00:09 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 17:00:09 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 17:00:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler.bin
12/29/2023 17:00:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 17:00:09 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 17:00:10 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_high_lr_linear_decay/step_0
12/29/2023 17:00:25 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_high_lr_linear_decay/step_0/model.safetensors
12/29/2023 17:00:25 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 17:00:25 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 17:00:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler.bin
12/29/2023 17:00:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 17:00:25 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_high_lr_linear_decay/step_0/random_states_0.pkl
12/29/2023 17:00:27 - INFO - accelerate.accelerator - Saving current state to exp_results/with_prompt_high_lr_linear_decay/step_0
12/29/2023 17:00:34 - INFO - accelerate.checkpointing - Model weights saved in exp_results/with_prompt_high_lr_linear_decay/step_0/model.safetensors
12/29/2023 17:00:34 - INFO - accelerate.checkpointing - Optimizer state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/optimizer.bin
12/29/2023 17:00:34 - INFO - accelerate.checkpointing - Scheduler state saved in exp_results/with_prompt_high_lr_linear_decay/step_0/scheduler.bin
12/29/2023 17:00:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler.bin
12/29/2023 17:00:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in exp_results/with_prompt_high_lr_linear_decay/step_0/sampler_1.bin
12/29/2023 17:00:34 - INFO - accelerate.checkpointing - Random states saved in exp_results/with_prompt_high_lr_linear_decay/step_0/random_states_0.pkl
  0%|                                                                                                                                  | 1/4605 [00:40<52:16:29, 40.88s/it]
Epoch: 0 | Step: 1 | Loss: 11.468478202819824
Epoch: 0 | Step: 1 | Loss: 11.471742630004883
Epoch: 0 | Step: 1 | Loss: 11.47018051147461
Epoch: 0 | Step: 1 | Loss: 11.470683097839355

  0%|                                                                                                                                  | 2/4605 [00:46<25:23:42, 19.86s/it]
Epoch: 0 | Step: 2 | Loss: 11.452966690063477
Epoch: 0 | Step: 2 | Loss: 11.457958221435547

  0%|                                                                                                                                  | 3/4605 [00:50<16:39:02, 13.03s/it]
Epoch: 0 | Step: 3 | Loss: 11.433927536010742
Epoch: 0 | Step: 3 | Loss: 11.437705039978027
Epoch: 0 | Step: 3 | Loss: 11.432554244995117
Epoch: 0 | Step: 3 | Loss: 11.436562538146973

  0%|                                                                                                                                  | 4/4605 [00:55<12:32:49,  9.82s/it]
  0%|                                                                                                                                  | 4/4605 [00:55<12:32:49,  9.82s/it]Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 918, in <module>
    main()
  File "/home/cwtang/david_dynamo/train_gsm8k_llama.py", line 716, in main
    outputs = model(**batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1073, in forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 100, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1181, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1068, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 796, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 386, in forward
    query_states = self.q_proj(hidden_states)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cwtang/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 311, in forward
    output = lora_B(lora_A(dropout(x)))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt