Train dataset size: 14732
Test dataset size: 819
Map: 100%|██████████████████████████████████████████████████████████████████████████| 15551/15551 [00:01<00:00, 15170.87 examples/s]
Max source length: 255
Max target length: 50
Map: 100%|██████████████████████████████████████████████████████████████████████████| 15551/15551 [00:00<00:00, 61339.34 examples/s]
Map:  68%|██████████████████████████████████████████████████▉                        | 10000/14732 [00:01<00:00, 7895.42 examples/s]
Keys of tokenized self.dataset: ['input_ids', 'attention_mask', 'labels']
{'adafactor': False,
 'adam_beta1': 0.9,
 'adam_beta2': 0.999,
 'adam_epsilon': 1e-08,
 'auto_find_batch_size': True,
 'bf16': False,
 'bf16_full_eval': False,
 'data_seed': None,
 'dataloader_drop_last': False,
 'dataloader_num_workers': 0,
 'dataloader_persistent_workers': False,
 'dataloader_pin_memory': True,
 'ddp_backend': None,
 'ddp_broadcast_buffers': None,
 'ddp_bucket_cap_mb': None,
 'ddp_find_unused_parameters': None,
 'ddp_timeout': 1800,
 'debug': [],
 'deepspeed': None,
 'disable_tqdm': False,
 'dispatch_batches': None,
 'do_eval': False,
 'do_predict': False,
 'do_train': False,
 'eval_accumulation_steps': None,
 'eval_delay': 0,
 'eval_steps': None,
 'evaluation_strategy': 'no',
 'fp16': False,
 'fp16_backend': 'auto',
 'fp16_full_eval': False,
 'fp16_opt_level': 'O1',
 'fsdp': [],
 'fsdp_config': {'min_num_params': 0,
                 'xla': False,
                 'xla_fsdp_grad_ckpt': False},
 'fsdp_min_num_params': 0,
 'fsdp_transformer_layer_cls_to_wrap': None,
 'full_determinism': False,
 'generation_config': None,
 'generation_max_length': None,
 'generation_num_beams': None,
 'gradient_accumulation_steps': 1,
 'gradient_checkpointing': False,
 'gradient_checkpointing_kwargs': None,
 'greater_is_better': None,
 'group_by_length': False,
 'half_precision_backend': 'auto',
 'hub_always_push': False,
 'hub_model_id': None,
 'hub_private_repo': False,
 'hub_strategy': 'every_save',
 'hub_token': '<HUB_TOKEN>',
 'ignore_data_skip': False,
 'include_inputs_for_metrics': False,
 'include_num_input_tokens_seen': False,
 'include_tokens_per_second': False,
 'jit_mode_eval': False,
 'label_names': None,
 'label_smoothing_factor': 0.0,
 'learning_rate': 0.001,
 'length_column_name': 'length',
 'load_best_model_at_end': False,
 'local_rank': 0,
 'log_level': 'passive',
 'log_level_replica': 'warning',
 'log_on_each_node': True,
 'logging_dir': 'test_lora-flan-t5-xxl-1e-3_linear_warm0.1_seed1234/logs',
 'logging_first_step': False,
 'logging_nan_inf_filter': True,
 'logging_steps': 1,
 'logging_strategy': 'steps',
 'lr_scheduler_kwargs': {},
 'lr_scheduler_type': 'linear',
 'max_grad_norm': 1.0,
 'max_steps': -1,
 'metric_for_best_model': None,
 'mp_parameters': '',
 'neftune_noise_alpha': None,
 'no_cuda': False,
 'num_train_epochs': 5,
 'optim': 'adamw_torch',
 'optim_args': None,
 'output_dir': 'test_lora-flan-t5-xxl-1e-3_linear_warm0.1_seed1234',
 'overwrite_output_dir': False,
 'past_index': -1,
 'per_device_eval_batch_size': 32,
 'per_device_train_batch_size': 32,
 'per_gpu_eval_batch_size': None,
 'per_gpu_train_batch_size': None,
 'predict_with_generate': False,
 'prediction_loss_only': False,
 'push_to_hub': False,
 'push_to_hub_model_id': None,
 'push_to_hub_organization': None,
 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',
 'ray_scope': 'last',
 'remove_unused_columns': True,
 'report_to': ['wandb'],
 'resume_from_checkpoint': None,
 'run_name': 'test_lora-flan-t5-xxl-1e-3_linear_warm0.1_seed1234',
 'save_on_each_node': False,
 'save_only_model': False,
 'save_safetensors': True,
 'save_steps': 500,
 'save_strategy': 'steps',
 'save_total_limit': None,
 'seed': 42,
 'skip_memory_metrics': True,
 'sortish_sampler': False,
 'split_batches': False,
 'tf32': None,
 'torch_compile': False,
 'torch_compile_backend': None,
 'torch_compile_mode': None,
 'torchdynamo': None,
 'tpu_metrics_debug': False,
 'tpu_num_cores': None,
 'use_cpu': False,
 'use_ipex': False,
 'use_legacy_prediction_loop': False,
 'use_mps_device': False,
 'warmup_ratio': 0.1,
 'warmup_steps': 0,
Map: 100%|███████████████████████████████████████████████████████████████████████████| 14732/14732 [00:01<00:00, 7514.37 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 4048.40 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 818/818 [00:00<00:00, 7905.81 examples/s]
Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████| 14732/14732 [00:00<00:00, 562580.68 examples/s]
Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████| 819/819 [00:00<00:00, 231618.57 examples/s]
Traceback (most recent call last):
  File "/home/cwtang/david_dynamo/flan_t5.py", line 253, in <module>
    flat_t5 = FlatT5()
  File "/home/cwtang/david_dynamo/flan_t5.py", line 95, in __init__
    self.setup_trainer()
  File "/home/cwtang/david_dynamo/flan_t5.py", line 217, in setup_trainer
    model=self.model,
AttributeError: 'FlatT5' object has no attribute 'model'